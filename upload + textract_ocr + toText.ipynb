{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import json, sys\r\n",
    "from pprint import pprint\r\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\r\n",
    "import logging\r\n",
    "from tqdm import tqdm\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#  just change these values\r\n",
    "sub_root_dir = r\"C:\\Users\\user\\Downloads\\textract\\subjects\"\r\n",
    "scans_root_dir = r'C:\\Users\\user\\Downloads\\textract\\scans'\r\n",
    "sub_codes = ['KCE051', 'KCE055', 'KCE502', 'KCE503', 'KNC501']\r\n",
    "scan_img_file_type = '.png'\r\n",
    "\r\n",
    "bucket_name = 'process-question-answer-scans'\r\n",
    "region_name='ap-south-1'\r\n",
    "aws_access_key_id = \"hakdugbua\"  # access to \"diabetic-retinopathy-data-from-radiology\" with read, write and list access\r\n",
    "aws_secret_access_key = \"hiaduhgiuadh\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%%time\r\n",
    "import boto3\r\n",
    "import botocore\r\n",
    "from botocore.exceptions import ClientError\r\n",
    "from collections import defaultdict\r\n",
    "\r\n",
    "def get_all_s3_keys(bucket, subdirectory, file_type=''):\r\n",
    "    keys = []\r\n",
    "    kwargs = {'Bucket': bucket, 'Prefix': subdirectory}\r\n",
    "    s3_client = boto3.client('s3',\r\n",
    "                         region_name=region_name,\r\n",
    "                         aws_access_key_id=aws_access_key_id,\r\n",
    "                         aws_secret_access_key=aws_secret_access_key)\r\n",
    "    while True:\r\n",
    "        resp = s3_client.list_objects_v2(**kwargs)\r\n",
    "        for obj in resp['Contents']:\r\n",
    "            key = obj['Key']\r\n",
    "            if key.endswith(file_type):\r\n",
    "                keys.append(key)\r\n",
    "        try:\r\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\r\n",
    "        except KeyError:\r\n",
    "            break\r\n",
    "    return keys\r\n",
    "\r\n",
    "def upload_to_s3(channel, file): \r\n",
    "    s3 = boto3.resource('s3',\r\n",
    "                         region_name=region_name,\r\n",
    "                         aws_access_key_id=aws_access_key_id,\r\n",
    "                         aws_secret_access_key=aws_secret_access_key) \r\n",
    "    data = open(file, \"rb\")\r\n",
    "    key = channel + '/' + file\r\n",
    "    print(\"Uploading file {} to s3://{}/{}\".format(file, bucket, channel))\r\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\r\n",
    "    \r\n",
    "def download(url):\r\n",
    "    filename = url.split(\"/\")[-1]\r\n",
    "    if not os.path.exists(filename):\r\n",
    "        urllib.request.urlretrieve(url, filename)\r\n",
    "\r\n",
    "def upload_dir_to_s3(bucket, s3_folder, dir_to_upload):\r\n",
    "    s3_client = boto3.client('s3',\r\n",
    "                             region_name=region_name,\r\n",
    "                             aws_access_key_id=aws_access_key_id,\r\n",
    "                             aws_secret_access_key=aws_secret_access_key)\r\n",
    "#     print(\"Uploading {} to s3://{}/{}\".format(dir_to_upload, bucket, s3_folder))\r\n",
    "    # enumerate local files recursively\r\n",
    "    for root, dirs, files in os.walk(dir_to_upload):\r\n",
    "        for filename in tqdm(files):\r\n",
    "            # construct the full local path\r\n",
    "            local_path = os.path.join(root, filename)\r\n",
    "            # construct the full Dropbox path\r\n",
    "            relative_path = os.path.relpath(local_path, dir_to_upload)\r\n",
    "            s3_path = os.path.join(s3_folder, relative_path).replace(\"\\\\\", \"/\")\r\n",
    "            try:\r\n",
    "                s3_client.head_object(Bucket=bucket, Key=s3_path)\r\n",
    "                print(\"Path found on S3! Deleting %s...\" % s3_path)\r\n",
    "                try:\r\n",
    "                    s3_client.delete_object(Bucket=bucket, Key=s3_path)\r\n",
    "                    try:\r\n",
    "#                         print(\"Uploading {} to s3://{}/{}\".format(dir_to_upload, bucket, s3_path)\r\n",
    "                        s3_client.upload_file(local_path, Bucket=bucket, Key=s3_path)\r\n",
    "                    except ClientError as e:\r\n",
    "                        logging.error(e)\r\n",
    "                except:\r\n",
    "                    print(\"Unable to delete from s3 %s...\" % s3_path)\r\n",
    "            except:\r\n",
    "                try:\r\n",
    "                    s3_client.upload_file(local_path, Bucket=bucket, Key=s3_path)\r\n",
    "                except ClientError as e:\r\n",
    "                    logging.error(e)\r\n",
    "    print(f\"Upload completed successfully to s3://{bucket}/{s3_folder}\")\r\n",
    "    \r\n",
    "def download_dir(s3_folder, local_path, bucket=\"\"):\r\n",
    "    client = boto3.client('s3', region_name=region_name)\r\n",
    "    keys = []\r\n",
    "    dirs = []\r\n",
    "    next_token = ''\r\n",
    "    base_kwargs = {\r\n",
    "        'Bucket': bucket,\r\n",
    "        'Prefix': s3_folder,\r\n",
    "    }\r\n",
    "    while next_token is not None:\r\n",
    "        kwargs = base_kwargs.copy()\r\n",
    "        if next_token != '':\r\n",
    "            kwargs.update({'ContinuationToken': next_token})\r\n",
    "        results = client.list_objects_v2(**kwargs)\r\n",
    "        contents = results.get('Contents')\r\n",
    "        for i in contents:\r\n",
    "            k = i.get('Key')\r\n",
    "            if k[-1] != '/':\r\n",
    "                keys.append(k)\r\n",
    "            else:\r\n",
    "                dirs.append(k)\r\n",
    "        next_token = results.get('NextContinuationToken')\r\n",
    "    for d in dirs:\r\n",
    "        dest_pathname = os.path.join(local_path, d)\r\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\r\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\r\n",
    "    print(\"{} files found in {} directories. Downloading now...\".format(len(keys), len(dirs)))\r\n",
    "    for k in tqdm(keys):\r\n",
    "        dest_pathname = os.path.join(local_path, k)\r\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\r\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\r\n",
    "        try:\r\n",
    "#             print(\"Downloading {}\".format(dest_pathname))\r\n",
    "            client.download_file(bucket, k, dest_pathname)\r\n",
    "        except botocore.exceptions.ClientError as e:\r\n",
    "            if e.response['Error']['Code'] == \"404\":\r\n",
    "                print(\"The object does not exist.\")\r\n",
    "            else:\r\n",
    "                raise\r\n",
    "    print(\"{} files downloaded successfully.\".format(len(keys)))\r\n",
    "def download_from_s3(s3_filename, local_path=\"0.fbx\"):\r\n",
    "    s3_client = boto3.client('s3',\r\n",
    "                             region_name=region_name,\r\n",
    "                             aws_access_key_id=aws_access_key_id,\r\n",
    "                             aws_secret_access_key=aws_secret_access_key)\r\n",
    "    print(\"Downloading file {} to {}\".format(s3_filename, local_path))\r\n",
    "    try:\r\n",
    "        s3_client.download_file(bucket, Key=s3_filename, Filename=local_path)\r\n",
    "        # it waits till the download completes then moves the execution to forward\r\n",
    "    except botocore.exceptions.ClientError as e:\r\n",
    "        if e.response['Error']['Code'] == \"404\":\r\n",
    "            print(\"The object does not exist.\")\r\n",
    "        else:\r\n",
    "            raise\r\n",
    "# upload_to_s3(\"pretrained\", 'se_resnext50_32x4d-a260b3a4.pth')\r\n",
    "# upload_dir_to_s3(bucket, s3_folder, dir_to_upload)\r\n",
    "\r\n",
    "# download('http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth')\r\n",
    "# download_dir(s3_folder='aptos-2015/test_images_768/', local_path='/home/ec2-user/SageMaker/data/aptos-2015/test_images_768/', bucket=bucket)\r\n",
    "# download_from_s3(s3_filename='pytorch-training-2020-12-29-09-38-13-247/source/sourcedir.tar.gz', local_path=\"/home/ec2-user/SageMaker/checkpoint/sourcedir.tar.gz\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 5.08 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Uploading scanned photos files to s3 for textraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove initial pages that contains introdcution, unless it creates the noise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for sub_code in sub_codes:\n",
    "    dir_to_upload = os.path.join(scans_root_dir, sub_code)\n",
    "    if(os.path.isdir(dir_to_upload)):\n",
    "        upload_dir_to_s3(bucket_name, s3_folder=sub_code, dir_to_upload=dir_to_upload)\n",
    "    else:\n",
    "        print(f\"{dir_to_upload} doesn't exists\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 162/162 [06:37<00:00,  2.45s/it]\n",
      "  0%|                                                                                                                        | 0/198 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Upload completed successfully to s3://process-question-answer-scans/KCE051\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 198/198 [19:14<00:00,  5.83s/it]\n",
      "  0%|                                                                                                                        | 0/195 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Upload completed successfully to s3://process-question-answer-scans/KCE055\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [20:23<00:00,  6.28s/it]\n",
      "  0%|                                                                                                                        | 0/170 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Upload completed successfully to s3://process-question-answer-scans/KCE502\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170/170 [11:57<00:00,  4.22s/it]\n",
      "  0%|                                                                                                                         | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Upload completed successfully to s3://process-question-answer-scans/KCE503\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [22:06<00:00, 18.69s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Upload completed successfully to s3://process-question-answer-scans/KNC501\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def textraction(sub_code):\n",
    "    lines = []\n",
    "    files = []   \n",
    "    textract = boto3.client(service_name='textract', \n",
    "                            region_name=region_name,\n",
    "                            aws_access_key_id=aws_access_key_id,\n",
    "                            aws_secret_access_key=aws_secret_access_key)\n",
    "    try:\n",
    "        files = get_all_s3_keys(bucket_name, subdirectory=sub_code, file_type=scan_img_file_type)\n",
    "        if files == []:\n",
    "            print(f'No file found in s3://{bucket_name}/{sub_code}, Upload scans first.')\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(\"Caught Exception\", e)\n",
    "        return\n",
    "    \n",
    "    txt_file_path = os.path.join(sub_root_dir, sub_code)+'.txt'\n",
    "    txt_file_opened = open(txt_file_path, 'w')\n",
    "\n",
    "    # detecting text in the documents and writing to txt\n",
    "    for s3_file_path in tqdm(files):\n",
    "        try:\n",
    "            response = textract.detect_document_text(\n",
    "                Document={\n",
    "                    \"S3Object\": {\n",
    "                        \"Bucket\": bucket_name,\n",
    "                        \"Name\": s3_file_path\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            for item in response[\"Blocks\"]:\n",
    "                if item[\"BlockType\"] == \"LINE\":\n",
    "                    txt_file_opened.write(item[\"Text\"]+'\\n')                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    txt_file_opened.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for sub_code in sub_codes:\n",
    "    textraction(sub_code=sub_code)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 162/162 [15:39<00:00,  5.80s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 198/198 [19:27<00:00,  5.90s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [13:39<00:00,  4.20s/it]\n",
      " 51%|████████████████████████████████████████████████████████▏                                                      | 86/170 [05:46<06:57,  4.97s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'charmap' codec can't encode character '\\u20b9' in position 40: character maps to <undefined>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r\n",
      " 51%|████████████████████████████████████████████████████████▊                                                      | 87/170 [05:51<06:44,  4.88s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'charmap' codec can't encode character '\\u20b9' in position 19: character maps to <undefined>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 86%|██████████████████████████████████████████████████████████████████████████████████████████████▍               | 146/170 [14:08<01:47,  4.48s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'charmap' codec can't encode character '\\u20b9' in position 6: character maps to <undefined>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████▍             | 149/170 [14:21<01:28,  4.19s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'charmap' codec can't encode character '\\u20b9' in position 21: character maps to <undefined>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████▎           | 152/170 [14:31<01:05,  3.64s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'charmap' codec can't encode character '\\u20b9' in position 10: character maps to <undefined>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170/170 [16:19<00:00,  5.76s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 71/71 [14:42<00:00, 12.43s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "# def quanify(sub_code, sub_name, drop_sem_str, isCamScanner):\n",
    "#     df = pd.DataFrame({\"question1\": [''],\n",
    "#                        'question2': [''],\n",
    "#                        'answer': ['']\n",
    "#                        })  \n",
    "#     txt_file_path = os.path.join(sub_root_dir, sub_code)+'.txt'\n",
    "\n",
    "#     with open(txt_file_path, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "\n",
    "#         contents = []\n",
    "#         # if a line conatains any of these, remove that\n",
    "#         drop_strs = ['QUESTIONS-ANSWERS', 'LONG ANSWER TYPE AND MEDIUM ANSWER TYPE QUESTIONS',\n",
    "#                     'VERY IMPORTANT QUESTIONS', 'FOLLOWING QUESTIONS ARE VERY IMPORTANT. THESE QUESTIONS',\n",
    "#                     'MAY BE ASKED IN YOUR SESSIONALS AS WELL AS', 'UNIVERSITY EXAMINATION',\n",
    "#                     'MARKS', '2 MARKS QUESTIONS', 'QUESTION', \n",
    "#                     'PART-1', 'PART-2', 'PART-3', 'PART-4', 'PART-5', 'PART-6', 'PART-7', 'PART-8', 'PART-9', 'PART-10', \n",
    "#                     'PART-11', 'PART-12', 'PART-13', 'PART-14', 'PART-15', 'PART-16', 'PART-17', 'PART-18', 'PART-19', 'PART-20',\n",
    "#                     'PART-21', 'PART-22', 'PART-23', 'PART-24', 'PART-25', 'PART-26', 'PART-27', 'PART-28', 'PART-29', 'PART-30',\n",
    "#                     'AKTU', 'MARKS 01', 'MARKS 02', 'MARKS 05', 'MARKS 10', 'MARKS 15', \n",
    "#                     '2010-11,', '2011-12,', '2012-13,', '2013-14,', '2014-15,', '2015-16,', '2016-17,', '2017-18,', '2018-19,', \n",
    "#                     '2019-20,', '2020-21,', '2021-22,', '2022-23,']\n",
    "\n",
    "#         question_end = ['.', '?', ',', '!']\n",
    "#         drop_after_qa_detn = ['QUE ', 'OR', 'ANSWER']  # Leading part of the Que line will also be removed.\n",
    "#         lines1 = []\n",
    "#         que1_line_start, que1_line_end, que2_line_start, que2_line_end, ans_line_start, ans_line_end = 0, 0, 0, 0, 0, 0\n",
    "#         que1_found, que2_found, ans_found = 0, 0, 0\n",
    "#         que_idx, ans_idx = 0, 0\n",
    "\n",
    "#         for line in lines:\n",
    "#             line = line.split('\\n')[0].upper()\n",
    "#             lines1.append(line)\n",
    "\n",
    "#         # appending all the dropping strings to one list\n",
    "#         drop_strs.append(drop_sem_str)\n",
    "#         if isCamScanner:\n",
    "#             drop_strs.append('SCANNED WITH CAMSCANNER')\n",
    "#         drop_strs.append(sub_name)\n",
    "#         for w in contents: drop_strs.append(w)\n",
    "\n",
    "#         lines = []\n",
    "        \n",
    "#         # removing all the sub-strings\n",
    "#         for line in lines1:\n",
    "#             found = 0\n",
    "#             for drop_str in drop_strs:\n",
    "#                 if line.find(drop_str) != -1: found = 1\n",
    "#             if found != 1: lines.append(line)\n",
    "#         del lines1\n",
    "        \n",
    "#         with open(os.path.join(sub_root_dir, \"txt_file_path.txt\"), 'w') as file:\n",
    "#             for line in lines:\n",
    "#                 file.writelines(line+\"\\n\")\n",
    "        \n",
    "#         start = True\n",
    "#         for i, line in enumerate(lines):\n",
    "#             if line.find('QUE') != -1 or line.find('QVE') != -1:\n",
    "#                 if start:\n",
    "#                     que1_found = 1\n",
    "#                     que1_line_start = i\n",
    "#                 else:\n",
    "#                     ans_line_end = i - 1\n",
    "#                     ###################### put the content in the dataframe\n",
    "#                     if (que1_found ==1) and (ans_found==1):\n",
    "#                         ###########  question1\n",
    "#                         question = lines[que1_line_start + 1]\n",
    "#                         if que1_line_start < que1_line_end:\n",
    "#                             for q in range(que1_line_end - que1_line_start - 1):\n",
    "#                                 que1_line_start += 1\n",
    "#                                 question += ' ' + lines[que1_line_start + 1]\n",
    "#                         que1_found = 0 # not set it here\n",
    "#                         df.at[que_idx, 'question1'] = question\n",
    "                        \n",
    "#                         if que2_found==1:\n",
    "#                             ###########  question2\n",
    "#                             question = lines[que2_line_start + 1]\n",
    "#                             if que2_line_start < que2_line_end:\n",
    "#                                 for q in range(que2_line_end - que2_line_start - 1):\n",
    "#                                     que2_line_start += 1\n",
    "#                                     question += ' ' + lines[que2_line_start + 1]\n",
    "#                             que2_found = 0\n",
    "#                             df.at[que_idx, 'question2'] = question\n",
    "                        \n",
    "#                         ########### answer\n",
    "#                         answer = lines[ans_line_start + 1]\n",
    "#                         if ans_line_start < ans_line_end:\n",
    "#                             for a in range(ans_line_end - ans_line_start - 1):\n",
    "#                                 ans_line_start += 1\n",
    "#                                 answer += ' ' + lines[ans_line_start + 1]\n",
    "#                         ans_found = 0\n",
    "#                         df.at[ans_idx, 'answer'] = answer\n",
    "#                         que_idx += 1\n",
    "#                         ans_idx += 1\n",
    "\n",
    "#                     que1_found = 1\n",
    "#                     que1_line_start = i\n",
    "#                 continue\n",
    "\n",
    "#             elif line == 'OR':\n",
    "#                 que1_line_end = i-1\n",
    "#                 que2_line_start = i\n",
    "#                 que2_found = 1\n",
    "#                 continue\n",
    "\n",
    "#             elif line.find('ANSWER') != -1 or line == 'ANSWER':\n",
    "#                 if que2_found ==1:\n",
    "#                     que2_line_end = i - 1\n",
    "#                 elif que1_found==1:\n",
    "#                     que1_line_end=i-1\n",
    "#                 ans_line_start = i\n",
    "#                 ans_found = 1\n",
    "#                 start=False\n",
    "#                 continue  \n",
    "#     return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QUANIFY"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "# sub_codes = ['KCE501', 'KCE051', 'KCE055', 'KCE502', 'KCE503', 'KNC501']\n",
    "# sub_names = ['GEOTECHNICAL ENGINEERING', 'CONCRETE TECHNOLOGY', 'INTRODUCTION TO HYDROLOGY', \n",
    "#             'STRUCTURAL ANALYSIS', 'QUANTITY ESTIMATION & MANAGEMENT', 'CONSTITUTION OF INDIA, LAW & ENGINEERING']\n",
    "# drop_sem_strs = ['CE-SEM-5', 'CE-SEM-5', 'CE-SEM-5', 'CE-SEM-4', 'CE-SEM-5', 'NCC-SEM-5 & 6']\n",
    "\n",
    "# for sub_code, sub_name, drop_sem_str in zip(sub_codes, sub_names, drop_sem_strs):\n",
    "# #     if sub_code=='KCE501':\n",
    "# #         continue\n",
    "#     df = quanify(sub_code, sub_name, drop_sem_str, isCamScanner=True)\n",
    "    \n",
    "#     csv_file_path = os.path.join(sub_root_dir, sub_code)+'.csv'\n",
    "#     df.to_csv(csv_file_path, index_label='id', sep='|')\n",
    "    \n",
    "#     print('QA csv file created: '+ csv_file_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE501.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE051.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE055.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE502.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE503.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KNC501.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "# import difflib\n",
    "# a = 'WHAT ARE THE ADVANTAGES AND DISADVANTAGES OF STEEL USED AS A STRUCTURAL MATERIAL ?'\n",
    "# b = 'WRITE ADVANTAGES OF STEEL USED AS A STRUCTURAL MATERIAL.'\n",
    "# seq = difflib.SequenceMatcher()\n",
    "# seq.set_seqs(a.lower(), b.lower()) # string b is x times similar to string a \n",
    "# d = seq.ratio()*100\n",
    "# print(d)\n",
    "\n",
    "\n",
    "# from difflib import get_close_matches\n",
    "\n",
    "# word_list =['fjadbbiu', 'jdbfag', 'nsfkubusi', 'bdjfh','gvhvh', 'nfd', 'njfd', a]\n",
    "# matches = get_close_matches('abcd', word_list, n=2, cutoff=0.1)\n",
    "# print(matches)\n",
    "\n",
    "\n",
    "# seq = difflib.SequenceMatcher()\n",
    "# seq.set_seqs('nsfkubusi', 'njfdu') # string b is x times similar to string a \n",
    "# d = seq.ratio()*100\n",
    "# print(d)\n",
    "\n",
    "\n",
    "# from fuzzywuzzy import process\n",
    "\n",
    "# q1_from_s3, similarity = process.extractOne(b, word_list)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "76.81159420289855\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "def quanify(sub_code, sub_name, drop_sem_str, isCamScanner):\n",
    "    df = pd.DataFrame({\"question1\": [''],\n",
    "                       'question2': [''],\n",
    "                       'answer': ['']\n",
    "                       })  \n",
    "    txt_file_path = os.path.join(sub_root_dir, sub_code)+'.txt'\n",
    "\n",
    "    with open(txt_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        contents = []\n",
    "        # if a line conatains any of these, remove that\n",
    "        drop_strs = ['QUESTIONS-ANSWERS', 'LONG ANSWER TYPE AND MEDIUM ANSWER TYPE QUESTIONS',\n",
    "                    'VERY IMPORTANT QUESTIONS', 'FOLLOWING QUESTIONS ARE VERY IMPORTANT. THESE QUESTIONS',\n",
    "                    'MAY BE ASKED IN YOUR SESSIONALS AS WELL AS', 'UNIVERSITY EXAMINATION',\n",
    "                    'MARKS', '2 MARKS QUESTIONS', 'QUESTION', \n",
    "                    'PART-1', 'PART-2', 'PART-3', 'PART-4', 'PART-5', 'PART-6', 'PART-7', 'PART-8', 'PART-9', 'PART-10', \n",
    "                    'PART-11', 'PART-12', 'PART-13', 'PART-14', 'PART-15', 'PART-16', 'PART-17', 'PART-18', 'PART-19', 'PART-20',\n",
    "                    'PART-21', 'PART-22', 'PART-23', 'PART-24', 'PART-25', 'PART-26', 'PART-27', 'PART-28', 'PART-29', 'PART-30',\n",
    "                    'AKTU', 'MARKS 01', 'MARKS 02', 'MARKS 05', 'MARKS 10', 'MARKS 15', \n",
    "                    '2010-11,', '2011-12,', '2012-13,', '2013-14,', '2014-15,', '2015-16,', '2016-17,', '2017-18,', '2018-19,', \n",
    "                    '2019-20,', '2020-21,', '2021-22,', '2022-23,']\n",
    "\n",
    "        question_end = ['.', '?', ',', '!']\n",
    "        drop_after_qa_detn = ['QUE ', 'OR', 'ANSWER']  # Leading part of the Que line will also be removed.\n",
    "        que1_line_start, que1_line_end, que2_line_start, que2_line_end, ans_line_start, ans_line_end = 0, 0, 0, 0, 0, 0\n",
    "        que1_found, que2_found, ans_found = 0, 0, 0\n",
    "        que_idx, ans_idx = 0, 0\n",
    "\n",
    "        lines1 = []\n",
    "        for line in lines:\n",
    "            line = line.split('\\n')[0].upper()\n",
    "            lines1.append(line)\n",
    "\n",
    "        # appending all the dropping strings to one list\n",
    "        drop_strs.append(drop_sem_str)\n",
    "        drop_strs.append(sub_name)\n",
    "        if isCamScanner:\n",
    "            drop_strs.append('SCANNED WITH CAMSCANNER')\n",
    "        for w in contents: drop_strs.append(w)\n",
    "        \n",
    "#         print(drop_strs)\n",
    "#         import re\n",
    "        lines = []\n",
    "        for line in lines1:\n",
    "            for drop_str in drop_strs:\n",
    "                line = line.replace(drop_str, \"\")\n",
    "#                 line = re.sub(' +', ' ', line)\n",
    "                line = \" \".join(line.split())  # removes more \"  \" from line string\n",
    "            lines.append(line)\n",
    "        del lines1\n",
    "        \n",
    "        with open(os.path.join(sub_root_dir, sub_code+\"txt_file_path.txt\"), 'w') as file:\n",
    "            for line in lines:\n",
    "                file.writelines(line+\"\\n\")\n",
    "        \n",
    "        start = True\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.find('QUE') != -1 or line.find('QVE') != -1:\n",
    "                if start:\n",
    "                    que1_found = 1\n",
    "                    que1_line_start = i\n",
    "                else:\n",
    "                    ans_line_end = i - 1\n",
    "                    ###################### put the content in the dataframe\n",
    "                    if (que1_found ==1) and (ans_found==1):\n",
    "                        ###########  question1\n",
    "                        question = lines[que1_line_start + 1]\n",
    "                        if que1_line_start < que1_line_end:\n",
    "                            for q in range(que1_line_end - que1_line_start - 1):\n",
    "                                que1_line_start += 1\n",
    "                                question += ' ' + lines[que1_line_start + 1]\n",
    "                        que1_found = 0 # not set it here\n",
    "                        df.at[que_idx, 'question1'] = question\n",
    "                        \n",
    "                        if que2_found==1:\n",
    "                            ###########  question2\n",
    "                            question = lines[que2_line_start + 1]\n",
    "                            if que2_line_start < que2_line_end:\n",
    "                                for q in range(que2_line_end - que2_line_start - 1):\n",
    "                                    que2_line_start += 1\n",
    "                                    question += ' ' + lines[que2_line_start + 1]\n",
    "                            que2_found = 0\n",
    "                            df.at[que_idx, 'question2'] = question\n",
    "                        \n",
    "                        ########### answer\n",
    "                        answer = lines[ans_line_start + 1]\n",
    "                        if ans_line_start < ans_line_end:\n",
    "                            for a in range(ans_line_end - ans_line_start - 1):\n",
    "                                ans_line_start += 1\n",
    "                                answer += ' ' + lines[ans_line_start + 1]\n",
    "                        ans_found = 0\n",
    "                        df.at[ans_idx, 'answer'] = answer\n",
    "                        que_idx += 1\n",
    "                        ans_idx += 1\n",
    "\n",
    "                    que1_found = 1\n",
    "                    que1_line_start = i\n",
    "                continue\n",
    "\n",
    "            elif line == 'OR':\n",
    "                que1_line_end = i-1\n",
    "                que2_line_start = i\n",
    "                que2_found = 1\n",
    "                continue\n",
    "\n",
    "            elif line.find('ANSWER') != -1 or line == 'ANSWER':\n",
    "                if que2_found ==1:\n",
    "                    que2_line_end = i - 1\n",
    "                elif que1_found==1:\n",
    "                    que1_line_end=i-1\n",
    "                ans_line_start = i\n",
    "                ans_found = 1\n",
    "                start=False\n",
    "                continue  \n",
    "                \n",
    "        ###########  question1\n",
    "        question = lines[que1_line_start + 1]\n",
    "        if que1_line_start < que1_line_end:\n",
    "            for q in range(que1_line_end - que1_line_start - 1):\n",
    "                que1_line_start += 1\n",
    "                question += ' ' + lines[que1_line_start + 1]\n",
    "        que1_found = 0 # not set it here\n",
    "        df.at[que_idx, 'question1'] = question\n",
    "\n",
    "        if que2_found==1:\n",
    "            ###########  question2\n",
    "            question = lines[que2_line_start + 1]\n",
    "            if que2_line_start < que2_line_end:\n",
    "                for q in range(que2_line_end - que2_line_start - 1):\n",
    "                    que2_line_start += 1\n",
    "                    question += ' ' + lines[que2_line_start + 1]\n",
    "            que2_found = 0\n",
    "            df.at[que_idx, 'question2'] = question\n",
    "\n",
    "        ########### answer\n",
    "        answer = lines[ans_line_start + 1]\n",
    "        if ans_line_start < ans_line_end:\n",
    "            for a in range(ans_line_end - ans_line_start - 1):\n",
    "                ans_line_start += 1\n",
    "                answer += ' ' + lines[ans_line_start + 1]\n",
    "        ans_found = 0\n",
    "        df.at[ans_idx, 'answer'] = answer\n",
    "            \n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "sub_codes = ['KCE501', 'KCE051', 'KCE055', 'KCE502', 'KCE503', 'KNC501']\n",
    "sub_names = ['GEOTECHNICAL ENGINEERING', 'CONCRETE TECHNOLOGY', 'INTRODUCTION TO HYDROLOGY', \n",
    "            'STRUCTURAL ANALYSIS', 'QUANTITY ESTIMATION & MANAGEMENT', 'CONSTITUTION OF INDIA, LAW & ENGINEERING']\n",
    "drop_sem_strs = ['CE-SEM-5', 'CE-SEM-5', 'CE-SEM-5', 'CE-SEM-4', 'CE-SEM-5', 'NCC-SEM-5 & 6']\n",
    "\n",
    "for sub_code, sub_name, drop_sem_str in zip(sub_codes, sub_names, drop_sem_strs):\n",
    "#     if sub_code=='KCE501':\n",
    "#         continue\n",
    "    df = quanify(sub_code, sub_name, drop_sem_str, isCamScanner=True)\n",
    "    \n",
    "    csv_file_path = os.path.join(sub_root_dir, sub_code)+'.csv'\n",
    "    df.to_csv(csv_file_path, index_label='id', sep='|')\n",
    "    \n",
    "    print('QA csv file created: '+ csv_file_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE501.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE051.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE055.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE502.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KCE503.csv\n",
      "QA csv file created: C:\\Users\\user\\Downloads\\textract\\subjects\\KNC501.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}